{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Friends.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPPMp_unw7DM",
        "colab_type": "text"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x7ToYjbwp8H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a63a1f9f-31be-49d2-9d52-8b6e06252150"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import json\n",
        "import os  # To access local files\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
        "from keras.layers import Flatten, Dropout\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from keras.models import Sequential\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icThjouLxB9e",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADo-x2kaxKTd",
        "colab_type": "text"
      },
      "source": [
        "##### Helping Functions\n",
        "- JSON 파일 읽기\n",
        "- invalid한 인코딩과 공백 등 제거\n",
        "- 불용어 제거 및 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x68GuyChyt0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def generateNewJson(jsonFileName, dialogs):  # Generate as newJsonFile with clean data\n",
        "    with open(jsonFileName + \"_new.json\", \"w\") as newJsonFile:\n",
        "        json.dump(dialogs, newJsonFile, indent=4, separators=(',', ': '))\n",
        "\n",
        "def loadDialogs(jsonFileName):\n",
        "    # https://www.fileformat.info/info/unicode/char/2014/index.htm\n",
        "    removeMap = {  # 위 링크를 참고해서 인코딩을 mapping함\n",
        "        \"\\x85\": \"…\",\n",
        "        \"\\x91\": \"'\",\n",
        "        \"\\x92\": \"'\",\n",
        "        \"\\x93\": \"\\\"\",\n",
        "        \"\\x94\": \"\\\"\",\n",
        "        \"\\x96\": \"-\",\n",
        "        \"\\x97\": \"-\",\n",
        "        \"\\xa0\": \"\",\n",
        "        \"\\xe8\": \"e\",\n",
        "        \"\\xe9\": \"e\",\n",
        "        \"\\u2014\": \"-\",\n",
        "        \"\\u2019\": \"'\",\n",
        "        \"\\u2026\": \"…\"\n",
        "    }\n",
        "\n",
        "    # Read JSON\n",
        "    with open(jsonFileName + \".json\") as jsonFile:\n",
        "        dialogs = json.load(jsonFile)\n",
        "\n",
        "    # Modify invalid encoding and remove redundant spaces\n",
        "    pattern = re.compile(r\"\\s+\")\n",
        "    for dialog in dialogs:\n",
        "        for speaking in dialog:\n",
        "            for key in removeMap:  # invalid한 인코딩을 바꿔줌\n",
        "                speaking[\"utterance\"] = speaking[\"utterance\"].replace(\n",
        "                    key, removeMap[key])\n",
        "            # 공백 제거 & lowercase로 변환\n",
        "            speaking[\"utterance\"] = (re.sub(pattern, \" \", speaking[\"utterance\"])).lower()\n",
        "            speaking[\"utterance\"] = re.sub(\"[^a-z ]\", \"\", speaking[\"utterance\"])\n",
        "\n",
        "    return dialogs\n",
        "\n",
        "def tokenizeDialogs(dialogs):\n",
        "    friendsStopwords = stopwords.words(\"english\")\n",
        "    for dialog in dialogs:\n",
        "        for speaking in dialog:\n",
        "            # print(speaking[\"utterance\"])  # 데이터를 확인하기 위해 command line에다 출려함\n",
        "            speaking[\"utterance\"] = nltk.regexp_tokenize(speaking[\"utterance\"], \"[\\w']+\")  # nltk와 정규표현식을 이용하여 토큰화\n",
        "            speaking[\"utterance\"] = [word for word in speaking['utterance'] if word not in friendsStopwords] # 불용어 제거\n",
        "    return dialogs\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVr3_tuLy0By",
        "colab_type": "text"
      },
      "source": [
        "- 원본 JSON파일을 preprocess해서 _new 파일 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thovSkXNzE5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# 새 JSON파일 생성\n",
        "path = os.getcwd() + \"/\"\n",
        "jsonFileNames = [\"friends_train\", \"friends_dev\", \"friends_test\"]\n",
        "for jsonFileName in jsonFileNames:\n",
        "    dialogs = loadDialogs(path + jsonFileName)\n",
        "    tokenizeDialogs(dialogs)\n",
        "    # 새 JSON파일 이름은 원래 이름 뒤에다가 \"_new\"를 붙여서 generate됨\n",
        "    generateNewJson(path + jsonFileName, dialogs) # 처리된 데이터들은 friends_xxx_new.json 파일에서 확인가능\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcN-gFFZzR5e",
        "colab_type": "text"
      },
      "source": [
        "##### DataFrame 만들기\n",
        "- `loadUtteranceSet()` - JSON파일 당 하나의 utteranceSet을 생성\n",
        "- `makeDateFrame()` - utteranceSet으로 dataFrame을 하나 씩 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrwcBVkg0u6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# preprocess된 json파일로부터 utterance와 emotion을 읽어옵니다\n",
        "def loadUtteranceSet(jsonFileName):  # Load clean dialogs from JSON\n",
        "    with open(jsonFileName + \".json\") as jsonFile:\n",
        "        dialogs = json.load(jsonFile)  # Read JSON\n",
        "\n",
        "        utteranceSet = []  # ONE utteranceSet for ONE file (dialog별로 구분되지 않습니다)\n",
        "        for dialog in dialogs:\n",
        "            for speaking in dialog:\n",
        "                utterance = []\n",
        "                utterance.append(speaking[\"utterance\"])\n",
        "                utterance.append(speaking[\"emotion\"])\n",
        "                utteranceSet.append(utterance)\n",
        "    return utteranceSet\n",
        "\n",
        "\n",
        "def makeDataFrame(utteranceSet):\n",
        "    # 두 column의 이름을 지정해주며 dataframe을 생성합니다\n",
        "    frame = pd.DataFrame(utteranceSet, columns=[\"utterance\", \"annotation\"])\n",
        "    return frame\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c47wVZNa0173",
        "colab_type": "text"
      },
      "source": [
        "- 새 JSON파일에 의해 dataframe을 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD8NIS3K8P1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "path = os.getcwd() + \"/\"\n",
        "jsonFileNames = [\"friends_dev_new\", \"friends_test_new\", \"friends_train_new\"]  # 읽고자 하는 파일 이름을 확장자 없이 여기에 넣으시면 됩니다\n",
        "\n",
        "frames = []  # ONE frame <- ONE utteranceSet <- ONE file\n",
        "for jsonFileName in jsonFileNames:\n",
        "    utteranceSet = loadUtteranceSet(path + jsonFileName)\n",
        "    # pandas로 frame 하나를 생성해서 리스트에 추가\n",
        "    frames.append(makeDataFrame(utteranceSet))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVWecJFa8ahB",
        "colab_type": "text"
      },
      "source": [
        "### 모델 제작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAR8zgcwvb04",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1286c5f6-a662-402c-9b67-19a28e55db7c"
      },
      "source": [
        "dev_data = frames[0]\n",
        "test_data = frames[1]\n",
        "train_data = frames[2]\n",
        "\n",
        "X_dev = dev_data[\"utterance\"].tolist()\n",
        "X_test = test_data[\"utterance\"].tolist()\n",
        "X_train = train_data[\"utterance\"].tolist()\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "threshold = 3\n",
        "total_cnt = len(tokenizer.word_index)  # 단어의 수\n",
        "rare_cnt = 0  # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0  # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0  # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거. 0번 패딩 토큰을 고려하여 +1\n",
        "vocab_size = total_cnt - rare_cnt + 1\n",
        "\n",
        "tokenizer = Tokenizer(vocab_size)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_dev = tokenizer.texts_to_sequences(X_dev)\n",
        "y_dev = np.array(dev_data['annotation'])\n",
        "y_train = np.array(train_data['annotation'])\n",
        "y_test = np.array(test_data['annotation'])\n",
        "\n",
        "\n",
        "# 빈 샘플 제거\n",
        "drop_train = [index for index, sentence in enumerate(\n",
        "    X_train) if len(sentence) < 1]\n",
        "drop_test = [index for index, sentence in enumerate(\n",
        "    X_test) if len(sentence) < 1]\n",
        "drop_dev = [index for index, sentence in enumerate(\n",
        "    X_dev) if len(sentence) < 1]\n",
        "\n",
        "X_train = np.delete(X_train, drop_train, axis=0)\n",
        "y_train = np.delete(y_train, drop_train, axis=0)\n",
        "\n",
        "X_test = np.delete(X_test, drop_test, axis=0)\n",
        "y_test = np.delete(y_test, drop_test, axis=0)\n",
        "\n",
        "X_dev = np.delete(X_dev, drop_dev, axis=0)\n",
        "y_dev = np.delete(y_dev, drop_dev, axis=0)\n",
        "\n",
        "# 패딩\n",
        "\n",
        "\n",
        "def below_threshold_len(max_len, nested_list):\n",
        "    cnt = 0\n",
        "    for s in nested_list:\n",
        "        if(len(s) <= max_len):\n",
        "            cnt = cnt + 1\n",
        "\n",
        "\n",
        "max_len = 30\n",
        "below_threshold_len(max_len, X_train)\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "X_dev = pad_sequences(X_dev, maxlen=max_len)\n",
        "\n",
        "\n",
        "def trans_y(y):\n",
        "    emotionMap = {\n",
        "        'neutral': 0,\n",
        "        'joy': 1,\n",
        "        'sadness': 2,\n",
        "        'fear': 3,\n",
        "        'anger': 4,\n",
        "        'surprise': 5,\n",
        "        'disgust': 6,\n",
        "        'non-neutral': 7\n",
        "    }\n",
        "    temp = []\n",
        "    for i in y:\n",
        "        temp.append(emotionMap.get(i, 'error'))\n",
        "    return temp\n",
        "\n",
        "\n",
        "y_dev = np.array(trans_y(y_dev))\n",
        "y_test = np.array(trans_y(y_test))\n",
        "y_train = np.array(trans_y(y_train))\n",
        "'''\n",
        "print(len(X_dev))\n",
        "print(len(y_dev))\n",
        "print(len(X_test))\n",
        "print(len(y_test))\n",
        "print(len(X_train))\n",
        "print(len(y_train))\n",
        "'''\n",
        "\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    y_pred = K.round(y_pred)\n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return K.mean(f1)\n",
        "\n",
        "\n",
        "def f1_loss(y_true, y_pred):\n",
        "\n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return 1 - K.mean(f1)\n",
        "\n",
        "\n",
        "# 순환 신경망 모델 생성\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(8, activation=\"softmax\"))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model_1.h5', monitor='val_acc',\n",
        "                     mode='max', verbose=1, save_best_only=True)\n",
        "model_json = model.to_json()\n",
        "with open(\"best_model_1.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss=f1_loss, metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, epochs=15, callbacks=[\n",
        "                    es, mc], batch_size=60, validation_split=0.1)\n",
        "\n",
        "\n",
        "# 컨볼루션 신경망 모델 생성\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 128, input_length=max_len))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(256,\n",
        "                 3,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(8, activation='softmax'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model_2.h5', monitor='val_acc',\n",
        "                     mode='max', verbose=1, save_best_only=True)\n",
        "model_json = model.to_json()\n",
        "with open(\"best_model_2.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss=f1_loss, metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, epochs=15, callbacks=[\n",
        "                    es, mc], batch_size=60, validation_split=0.1)\n",
        "\n",
        "\n",
        "# 순환 컨볼루션 신경망 모델\n",
        "model = Sequential()\n",
        "model.add(Embedding(20000, 128, input_length=max_len))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(256,\n",
        "                 3,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(MaxPooling1D(pool_size=4))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(8, activation='softmax'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model_3.h5', monitor='val_acc',\n",
        "                     mode='max', verbose=1, save_best_only=True)\n",
        "model_json = model.to_json()\n",
        "with open(\"best_model_3.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss=f1_loss, metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, epochs=15, callbacks=[\n",
        "                    es, mc], batch_size=60, validation_split=0.1)\n",
        "\n",
        "\n",
        "json_file = open(\"best_model_1.json\", \"r\")\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "loaded_model.load_weights(\"best_model_1.h5\")\n",
        "loaded_model.compile(optimizer='rmsprop',\n",
        "                     loss=f1_loss, metrics=['acc'])\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))\n",
        "\n",
        "json_file = open(\"best_model_2.json\", \"r\")\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "loaded_model.load_weights(\"best_model_2.h5\")\n",
        "loaded_model.compile(optimizer='rmsprop',\n",
        "                     loss=f1_loss, metrics=['acc'])\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))\n",
        "\n",
        "json_file = open(\"best_model_3.json\", \"r\")\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "loaded_model.load_weights(\"best_model_3.h5\")\n",
        "loaded_model.compile(optimizer='rmsprop',\n",
        "                     loss=f1_loss, metrics=['acc'])\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 8894 samples, validate on 989 samples\n",
            "Epoch 1/15\n",
            "8894/8894 [==============================] - 13s 1ms/step - loss: 0.7623 - acc: 0.1718 - val_loss: 0.7620 - val_acc: 0.0688\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.06876, saving model to best_model_1.h5\n",
            "Epoch 2/15\n",
            "8894/8894 [==============================] - 12s 1ms/step - loss: 0.7623 - acc: 0.1610 - val_loss: 0.7620 - val_acc: 0.0688\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.06876\n",
            "Epoch 3/15\n",
            "8894/8894 [==============================] - 12s 1ms/step - loss: 0.7623 - acc: 0.1527 - val_loss: 0.7620 - val_acc: 0.0344\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.06876\n",
            "Epoch 4/15\n",
            "8894/8894 [==============================] - 12s 1ms/step - loss: 0.7623 - acc: 0.1543 - val_loss: 0.7620 - val_acc: 0.1527\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.06876 to 0.15268, saving model to best_model_1.h5\n",
            "Epoch 5/15\n",
            "8894/8894 [==============================] - 12s 1ms/step - loss: 0.7623 - acc: 0.1526 - val_loss: 0.7620 - val_acc: 0.1850\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.15268 to 0.18504, saving model to best_model_1.h5\n",
            "Epoch 6/15\n",
            "8894/8894 [==============================] - 12s 1ms/step - loss: 0.7623 - acc: 0.1476 - val_loss: 0.7620 - val_acc: 0.0849\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.18504\n",
            "Epoch 7/15\n",
            "8894/8894 [==============================] - 12s 1ms/step - loss: 0.7624 - acc: 0.1525 - val_loss: 0.7620 - val_acc: 0.0435\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.18504\n",
            "Epoch 8/15\n",
            "8894/8894 [==============================] - 12s 1ms/step - loss: 0.7623 - acc: 0.1557 - val_loss: 0.7620 - val_acc: 0.1537\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.18504\n",
            "Epoch 9/15\n",
            "8894/8894 [==============================] - 12s 1ms/step - loss: 0.7623 - acc: 0.1510 - val_loss: 0.7620 - val_acc: 0.2123\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.18504 to 0.21234, saving model to best_model_1.h5\n",
            "Epoch 10/15\n",
            "8894/8894 [==============================] - 12s 1ms/step - loss: 0.7623 - acc: 0.1656 - val_loss: 0.7620 - val_acc: 0.0546\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.21234\n",
            "Epoch 11/15\n",
            "8894/8894 [==============================] - 12s 1ms/step - loss: 0.7623 - acc: 0.1593 - val_loss: 0.7620 - val_acc: 0.0263\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.21234\n",
            "Epoch 12/15\n",
            "8894/8894 [==============================] - 12s 1ms/step - loss: 0.7623 - acc: 0.1615 - val_loss: 0.7620 - val_acc: 0.3448\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.21234 to 0.34479, saving model to best_model_1.h5\n",
            "Epoch 00012: early stopping\n",
            "Train on 8894 samples, validate on 989 samples\n",
            "Epoch 1/15\n",
            "8894/8894 [==============================] - 5s 573us/step - loss: 0.7623 - acc: 0.2113 - val_loss: 0.7620 - val_acc: 0.2194\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.21941, saving model to best_model_2.h5\n",
            "Epoch 2/15\n",
            "8894/8894 [==============================] - 5s 538us/step - loss: 0.7622 - acc: 0.2810 - val_loss: 0.7620 - val_acc: 0.0172\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.21941\n",
            "Epoch 3/15\n",
            "8894/8894 [==============================] - 5s 535us/step - loss: 0.7623 - acc: 0.1936 - val_loss: 0.7620 - val_acc: 0.5490\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.21941 to 0.54904, saving model to best_model_2.h5\n",
            "Epoch 4/15\n",
            "8894/8894 [==============================] - 5s 535us/step - loss: 0.7623 - acc: 0.1746 - val_loss: 0.7620 - val_acc: 0.0839\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.54904\n",
            "Epoch 5/15\n",
            "8894/8894 [==============================] - 5s 546us/step - loss: 0.7623 - acc: 0.1802 - val_loss: 0.7620 - val_acc: 0.0293\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.54904\n",
            "Epoch 6/15\n",
            "8894/8894 [==============================] - 5s 541us/step - loss: 0.7623 - acc: 0.1459 - val_loss: 0.7620 - val_acc: 0.0404\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.54904\n",
            "Epoch 7/15\n",
            "8894/8894 [==============================] - 5s 544us/step - loss: 0.7623 - acc: 0.1436 - val_loss: 0.7620 - val_acc: 0.0404\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.54904\n",
            "Epoch 8/15\n",
            "8894/8894 [==============================] - 5s 540us/step - loss: 0.7623 - acc: 0.1342 - val_loss: 0.7620 - val_acc: 0.0061\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.54904\n",
            "Epoch 9/15\n",
            "8894/8894 [==============================] - 5s 542us/step - loss: 0.7623 - acc: 0.1255 - val_loss: 0.7620 - val_acc: 0.0111\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.54904\n",
            "Epoch 10/15\n",
            "8894/8894 [==============================] - 5s 534us/step - loss: 0.7623 - acc: 0.1172 - val_loss: 0.7620 - val_acc: 0.0162\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.54904\n",
            "Epoch 00010: early stopping\n",
            "Train on 8894 samples, validate on 989 samples\n",
            "Epoch 1/15\n",
            "8894/8894 [==============================] - 12s 1ms/step - loss: 0.7623 - acc: 0.1082 - val_loss: 0.7620 - val_acc: 0.2305\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.23054, saving model to best_model_3.h5\n",
            "Epoch 2/15\n",
            "8894/8894 [==============================] - 12s 1ms/step - loss: 0.7623 - acc: 0.1088 - val_loss: 0.7620 - val_acc: 0.0243\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.23054\n",
            "Epoch 3/15\n",
            "8894/8894 [==============================] - 11s 1ms/step - loss: 0.7623 - acc: 0.0924 - val_loss: 0.7620 - val_acc: 0.5632\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.23054 to 0.56320, saving model to best_model_3.h5\n",
            "Epoch 4/15\n",
            "8894/8894 [==============================] - 11s 1ms/step - loss: 0.7623 - acc: 0.1091 - val_loss: 0.7620 - val_acc: 0.0040\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.56320\n",
            "Epoch 5/15\n",
            "8894/8894 [==============================] - 11s 1ms/step - loss: 0.7623 - acc: 0.1141 - val_loss: 0.7620 - val_acc: 0.0607\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.56320\n",
            "Epoch 6/15\n",
            "8894/8894 [==============================] - 11s 1ms/step - loss: 0.7623 - acc: 0.1301 - val_loss: 0.7620 - val_acc: 0.0404\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.56320\n",
            "Epoch 7/15\n",
            "8894/8894 [==============================] - 12s 1ms/step - loss: 0.7623 - acc: 0.1220 - val_loss: 0.7620 - val_acc: 0.0061\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.56320\n",
            "Epoch 8/15\n",
            "8894/8894 [==============================] - 12s 1ms/step - loss: 0.7623 - acc: 0.1656 - val_loss: 0.7620 - val_acc: 0.8544\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.56320 to 0.85440, saving model to best_model_3.h5\n",
            "Epoch 9/15\n",
            "8894/8894 [==============================] - 12s 1ms/step - loss: 0.7624 - acc: 0.1281 - val_loss: 0.7620 - val_acc: 0.0718\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.85440\n",
            "Epoch 00009: early stopping\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.7635 - acc: 0.2343\n",
            "\n",
            " 테스트 정확도: 0.2343\n",
            "81/81 [==============================] - 0s 6ms/step - loss: 0.7635 - acc: 0.2788\n",
            "\n",
            " 테스트 정확도: 0.2788\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.7635 - acc: 0.4242\n",
            "\n",
            " 테스트 정확도: 0.4242\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}